\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage {amsmath , amssymb , amsthm }
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{cancel}
\theoremstyle{definition}
\newtheorem{defi}{Definition}[section]
\newtheorem*{notat}{Notation}
\newtheorem{exi}{Example}[section]
\newtheorem{teo}{Theorem}[section]
\newtheorem*{rem}{Remark}
\newcommand{\indep}{\perp \!\!\! \perp}

\usepackage{graphicx}
\graphicspath{ {./images/} } % Path relative to the main .tex file 
\usepackage{float}
\usepackage{caption}

\newcommand{\fg}[3][\relax]{%
  \begin{figure}[H]%[htp]%
    \centering
    \captionsetup{width=0.7\textwidth}
      \includegraphics[width = #2\textwidth]{#3}%
      \ifx\relax#1\else\caption{#1}\fi
      \label{#3}
  \end{figure}%
  %\FloatBarrier%
}

\begin{document}
	%\maketitle	Martingales
	\section{Conditional Expectation}
%	\begin{Prop}
		$(\Omega,\mathcal{F},\mathbb{P})$  probability space. \\
		$\mathcal{G}\subseteq\mathcal{F}$           $ \mathcal{G}$ sub-$\sigma$-algebra of  $\mathcal{F} $ .\\
		$ X:\Omega\to\mathbb{R} $ random variable with $ \mathbb{E}[|X|]<+\infty .$
		\begin{defi}
			We call conditional expectatiom of $X$ w.r.t. $\mathcal{G}$ any $\mathcal{G}$-measurable random variable $ V $ s.t.:
			\begin{equation}
				\int_{G}X\mathrm{d}\mathbb{P} = \int_{G}V\mathrm{d}\mathbb{P}  \qquad    \forall G \in \mathcal{G}
			\end{equation}
		\end{defi}
		\begin{notat}
			$V=\mathbb{E}[X|\mathcal{G}]$.
		\end{notat}
	\begin{exi}
		$X_1...X_n$  $ B(1,p) $ independent   $ S_n= X_1+...+X_n$ \\
		$\mathcal{G}=\sigma(S_n)$ $\sigma$-algebra generated by the events $ \{S_n=k\}$   $ k=0...n$ 
		\begin{equation*}
			\forall i\in {1...n}  \quad  \mathbb{E}[X_i|\sigma(S_n)]=\frac{S_n}{n}  \left(\text{another notation } \mathbb{E}[X_i|S_n=k]=\frac{k}{n}\right) 
		\end{equation*}
	\end{exi}
\begin{proof}
Any set in $\mathcal{G}$ is union of $ {S_n=k} $ for some $ k $'s. Therefore it suffices to show that:
\begin{equation*}
	\int_{{S_n=k}}X_i\mathrm{d}\mathbb{P} = \int_{{S_n=k}}\frac{S_n}{n}\mathrm{d}\mathbb{P}  \qquad    \forall k=0...n
\end{equation*}
$ X_i  $ has values in $ \{0,1\} $ so
\begin{equation*}
	\int_{\{S_n=k\}}\mathbb{I}_{\{X_i=1\}}\mathrm{d}\mathbb{P} = \frac{k}{n}\cdot\mathbb{P}\{S_n=k\}   \qquad   \forall k=0...n
\end{equation*}
We call $\hat{S}_n=X_1+...+X_{i-1}+X_{i+1}+...X_n$, then:
\begin{equation*}
	\begin{split}
	\int_{\{S_n=k\}}\mathbb{I}_{X_i}\mathrm{d}\mathbb{P} & = \mathbb{P}\{S_n=k,X_i=1\}\\
	&=\mathbb{P}\{\hat{S}_n=k-1,X_i=1\}\\
	&=\mathbb{P}\{\hat{S}_n=k-1\}\mathbb{P}\{X_i=1\}\\
	  & =\binom{n-1}{k-1}p^{k-1}(1-p)^{n-k}p 
	  \qquad\forall k=0...n
   \end{split}
\end{equation*}
We have to check that
\begin{equation*}
	\binom{n-1}{k-1}p^{k}(1-p)^{n-k}=\frac{k}{n}\binom{n}{k}p^{k}(1-p)^{n-k}\left(=\frac{k}{n}\mathbb{P}\{S_n=k\}\right)
\end{equation*}
indeed
\begin{equation*}
	\frac{k}{n}\binom{n}{k}=\frac{k}{n}\frac{n!}{k!(n-k)!}=\binom{n-1}{k-1}.
\end{equation*}
Then
\begin{equation*}
	\int_{{S_n=k}}X_i\mathrm{d}\mathbb{P} = \int_{{S_n=k}}\frac{S_n}{n}\mathrm{d}\mathbb{P}  \qquad    \forall k=0...n
\end{equation*}
So
\begin{equation*}
	\mathbb{E}[X_i|\sigma(S_n)]=\frac{S_n}{n}
\end{equation*}

\end{proof}
	\begin{exi}
		$ X,Y $ real random variable with joint distribution $ f_{X,Y} $ and conditional distribution:
		\begin{equation*}
			f_{Y|X}(y|x)=
			\begin{cases}
				\frac{f_{X,Y}(x,y)}{f_X(x)}   &\text{if } f_X(x)>0 \\
				0 & \text{otherwise}
			\end{cases}
		\end{equation*}
	Then
	\begin{equation*}
		\begin{split}
			\mathbb{E}[Y|\sigma(X)]=\int y \cdot f_{Y|X}(y|X)\mathrm{d}y \\
			\mathrm{r.v.} \quad \omega\to\int y\cdot f_{Y|X}(y|X(\omega))\mathrm{d}y
		\end{split}	
	\end{equation*}
	\end{exi}
\begin{teo}
	Given $ X $ with $\mathbb{E}[|X|]<+\infty$ and $ \mathcal{G}$ sub-$\sigma$-algebra of  $\mathcal{F} $, one can always find $\mathbb{E}[X|\mathcal{G}]$.
\end{teo}
\begin{proof}
	Suppose $X\geq0$ (if not we write $X=X^{+}-X^{-}$ and work on the two parts separately).\\
	Consider the following measures on $\mathcal{G}$: \\
	\begin{itemize}
		\item $ \mathbb{P}_{|\mathcal{G}} $ ($\mathbb{P}$ is defined on $ \mathcal{F} $). \\
		\item $ \mathbb{Q}(G)=\int_{G}X\mathrm{d}\mathbb{P} $. \\
	\end{itemize}
	Note that $\mathbb{Q}$ is absolutely continuous w.r.t $\mathbb{P}_{|\mathcal{G}}$ ($\mathbb{Q}\ll\mathbb{P}_{|\mathcal{G}}$ ). \\
	Then, by Radon-Nykodim theorem, there exists the density $V$ of $\mathbb{Q}$ w.r.t. $\mathbb{P}_{|\mathcal{G}}$ $ \left(V=\frac{\mathrm{d}\mathbb{Q}}{\mathrm{d}\mathbb{P}_{|\mathcal{G}}}\right). $ \\
	$V$ is $\mathcal{G}$-measurable because $\mathbb{Q}$ and $\mathbb{P}_{|\mathcal{G}}$ are measures on $\mathcal{G}$.\\
	Then:
	\begin{equation*}
		\mathbb{Q}(G)=\int_{G}X\mathrm{d}\mathbb{P}=\int_{G}V\mathrm{d}\mathbb{P}
	\end{equation*}
\end{proof}
\begin{exi}
	$(X_n)_{n>0}$ Markov chain with state space $I$ and transition matrix $P$.\\
	$f:I\to\mathbb{R} \quad$s.t.$\quad\mathbb{E}[|f(X_n)|]<+\infty \qquad \forall n\in\mathbb{N}$.\\ Then:
	\begin{equation*}
		\mathbb{E}[f(X_{n+1})|\sigma(X_1,...,X_n)]=(Pf)(X_n)
	\end{equation*}
Where $(Pf)(j)=\sum_{j'\in I}p_{jj'}f(j')$.
\begin{proof}
	We want to show that $\forall G \in \sigma(X_1,...,X_n)$
	\begin{equation*}
		\int_{G}f(X_{n+1})\mathrm{d}\mathbb{P}=\int_{G}(Pf)(X_n)\mathrm{d}\mathbb{P}
	\end{equation*}
Each $G$ is the countable union of a set ${X_1=i_1,...,X_n=i_n}$, therefore we must show that:
\begin{equation*}
	\int_{\{X_1=i_1,...,X_n=i_n\}}f(X_{n+1})\mathrm{d}\mathbb{P}=\int_{\{X_1=i_1,...,X_n=i_n\}}(Pf)(X_n)\mathrm{d}\mathbb{P}
\end{equation*}
The LHS can be written as follows:
\begin{equation*}
	\begin{split}
		\int_{\{X_1=i_1,...,X_n=i_n\}}f(X_{n+1})\mathrm{d}\mathbb{P}&=\sum_{j\in I}\int_{\{X_1=i_1,...,X_n=i_n\}}f(j)\mathbb{I}_{\{X_{n+1}\}}\mathrm{d}\mathbb{P} \\
		&=\sum_{j\in I}f(j)\mathbb{P}\{X_{n+1}=j,X_n=i_n,...,X_1=i_1\}\\
		&=\sum_{j\in I}f(j)\mathbb{P}\{X_{n+1}=j|X_n=i_n,\cancel{...}\}\mathbb{P}\{X_n=i_n,...,X_1=i_1\} \\
		&=\sum_{j\in I}f(j)p_{i_n j}\int_{\{X_1=i_1,...,X_n=i_n\}}\mathrm{d}\mathbb{P} \\
		&=\int_{\{X_1=i_1,...,X_n=i_n\}}\sum_{j\in I}f(j)p_{X_n j}\mathrm{d}\mathbb{P} \\
		&=\int_{\{X_1=i_1,...,X_n=i_n\}}(Pf)(X_n)\mathrm{d}\mathbb{P}
	\end{split}
\end{equation*}
\end{proof}
\end{exi}
\begin{rem}[for CTMC]
	\begin{equation*}
		s<t \qquad \mathbb{E}[f(X_t)|\sigma(X_r|r\leq s)]=(P_{t-s}f)(X_s)
	\end{equation*}
\end{rem}
\subsection{Properties of $\mathbb{E}[\, \cdot\, |\mathcal{G}]$}
%\subsubsection{(linear)}
\begin{enumerate}
	\item \textbf{(linear)} $\mathbb{E}[\alpha X+\beta Y|\mathcal{G}] = \alpha\cdot\mathbb{E}[X|\mathcal{G}]+\beta\cdot\mathbb{E}[Y|\mathcal{G}]$
	\item \textbf{(positive)} $X\geq 0 \ \implies \ \mathbb{E}[X|\mathcal{G}]\geq 0$
	\item \textbf{(normalized)} $\mathbb{E}[1|\mathcal{G}]=1$
	\item \textbf{Projective Property} 
	\begin{teo}
		If $ \mathcal{H}$ is a sub-$\sigma$-algebra of  $\mathcal{G}$ then:
		\begin{equation*}
		\mathbb{E}[\, \mathbb{E}[X|\mathcal{G}]\, |\,\mathcal{H}\,]=\mathbb{E}[ X |\mathcal{H}]
		\end{equation*}
	\end{teo}
\begin{proof}
	We must check that $\forall H \in\mathcal{H}$
	\begin{equation*}
		\int_{H}\mathbb{E}[\, \mathbb{E}[X|\mathcal{G}]\, |\,\mathcal{H}\,]\mathrm{d}\mathbb{P}=	
		\int_{H}\mathbb{E}[X|\mathcal{H}]\mathrm{d}\mathbb{P}
	\end{equation*}
Since $H\in \mathcal{H}$ 
\begin{equation*}
	\int_{H}\mathbb{E}[X|\mathcal{H}]\mathrm{d}\mathbb{P}=\int_{H}X\mathrm{d}\mathbb{P}
\end{equation*}
and since $\mathcal{H}\subseteq\mathcal{G}$, $H\in\mathcal{G}$, so:
\begin{equation*}
		\int_{H}\mathbb{E}[\, \mathbb{E}[X|\mathcal{G}]\, |\,\mathcal{H}\,]\mathrm{d}\mathbb{P}=	\int_{H}\mathbb{E}[X|\mathcal{G}]\mathrm{d}\mathbb{P}=\int_{H}X\mathrm{d}\mathbb{P}
\end{equation*}
\end{proof}
\begin{defi}
	$X$ real r.v. with $\mathbb{E}[|X|]<+\infty $,  $ \mathcal{G}$ sub-$\sigma$-algebra of  $\mathcal{F} $ 
	\begin{equation*}
		X \indep \mathcal{G} \ \iff \ \mathbb{P}\left(\{X\in B\}\cap G\right)=\mathbb{P}\{X\in B\}\cdot \mathbb{P}\{G\} \qquad \forall B \in \mathcal{B}(\mathbb{R}) \quad \forall G\in  \mathcal{G}
	\end{equation*}
\end{defi}
\begin{rem}
	If $\mathcal{G}=\sigma(Y)$ the definition matches with $X \indep Y$ becaus every $ G\in  \mathcal{G} $ is in the form $\{Y\in B'\}$ with $B'\in \mathcal{B}(\mathbb{R})$.
\end{rem}
\begin{teo}
	 $X \indep \mathcal{G} \implies \mathbb{E}[X|\mathcal{G}]=\mathbb{E}[X]$
\end{teo}
\begin{proof}
	The constant r.v. $ \mathbb{E}[X] $ is $\mathcal{G}$-measurable $ \forall G\in  \mathcal{G} $ \\
	Note that $ \forall B\in  \mathcal{B}(\mathbb{R}) $
	\begin{equation*}
		\begin{split}
			\int_{G}\mathbb{I}_B (X)\mathrm{d}\mathbb{P}&=\mathbb{P}\left(\{X\in B\}\cap G\right)=\mathbb{P}\{X\in B\}\cdot \mathbb{P}\{G\}=\\
			&=\int_{G}\mathbb{P}\{X\in B\}\mathrm{d}\mathbb{P}=\int_{G}\mathbb{E}[\mathbb{I}_B (X)]\mathrm{d}\mathbb{P}
		\end{split}		
	\end{equation*}
Then $\forall f:\mathbb{R}\to\mathbb{R}$ measurable and bounded
\begin{equation*}
	\int_{G}f (X)\mathrm{d}\mathbb{P}=\int_{G}\mathbb{E}[f (X)]\mathrm{d}\mathbb{P}
\end{equation*}
Hence we can construct a sequence of boundend functions to approximate the identity $x\to x$, defined as:
\begin{equation*}
	f_n(x)=\max \{-n,\min \{x,n\}\}
\end{equation*}
\fg[Approximation of the identity map.]{0.3}{identity_approximation}
s.t.:
\begin{equation*}
	\int_{G}f_n (X)\mathrm{d}\mathbb{P}=\int_{G}\mathbb{E}[f_n (X)]\mathrm{d}\mathbb{P}
\end{equation*}
Finally, applying the monotone convergence theorem as $n\to\infty$
\begin{equation*}
	\int_{G}X\mathrm{d}\mathbb{P}=\int_{G}\mathbb{E}[X]\mathrm{d}\mathbb{P}
\end{equation*}
\end{proof}
\item \textbf{Contractivity in $\mathcal{L}^p(\Omega,\mathcal{F},\mathbb{P})$}, $p\geq 1$ \\
\begin{equation*}
	X \in \mathcal{L}^p(\Omega,\mathcal{F},\mathbb{P})\implies \mathbb{E}[X|\mathcal{G}]\in \mathcal{L}^p(\Omega,\mathcal{G},\mathbb{P})
\end{equation*}
Indeed, is a consequence of the Jensen's inequality:
If $\Phi : \mathbb{R}\to\mathbb{R}$ is convex and $\mathbb{E}[|\Phi(X)|]<+\infty $, then:
\begin{equation*}
\Phi(\mathbb{E}[X])\leq\mathbb{E}[\Phi(X)]
\end{equation*}
(that also holds for conditional expectations $\Phi(\mathbb{E}[X|\mathcal{G}])\leq\mathbb{E}[\Phi(X)|\mathcal{G}]$). In our case $ \Phi(X)=|X|^p$.
\begin{teo}
	$X$ r.v. with $\mathbb{E}[|X|^p]<+\infty $\\
	$V$ $\mathcal{G}$-measurable random variable with $\mathbb{E}[|V|^q]<+\infty$ with $\frac{1}{p}+\frac{1}{q}=1$ conjugate index.
	Then
	\begin{equation*}
		\mathbb{E}[V\cdot X|\mathcal{G}]=V\cdot \mathbb{E}[X|\mathcal{G}]
	\end{equation*}
\end{teo}
\begin{proof}
	$V$ indicator function $V=\mathbb{I}_G \quad G\in\mathcal{G}$ ($V$ $\mathcal{G}$-measurable), then, by definition of conditional expectation, $\forall G'\in \mathcal{G}$:
\begin{equation*}
	\begin{split}
		\int_{G'}\mathbb{E}[\mathbb{I}_G \cdot X|\mathcal{G}]\mathrm{d}\mathbb{P}&=
		\int_{G'}\mathbb{I}_G \cdot X\mathrm{d}\mathbb{P}=
		\int_{G'\cap G}X\mathrm{d}\mathbb{P}=\\
		&=\int_{G'\cap G}\mathbb{E}[X|\mathcal{G}]\mathrm{d}\mathbb{P}=
		\int_{G'}\mathbb{I}_G \cdot\mathbb{E}[ X|\mathcal{G}]\mathrm{d}\mathbb{P}
	\end{split}
\end{equation*}
by the arbitrarity of $G'$: 
	\begin{equation*}
	\mathbb{E}[\mathbb{I}_G\cdot X|\mathcal{G}]=\mathbb{I}_G\cdot \mathbb{E}[X|\mathcal{G}]
\end{equation*}
by linearity:
\begin{equation*}
	\mathbb{E}[V\cdot X|\mathcal{G}]=V\cdot \mathbb{E}[X|\mathcal{G}] \qquad\forall V \text{simple and  $\mathcal{G}$-measurable}  
\end{equation*} 
If $V>0$, $V=\sup_{n\geq 1}V_n$ with $V_n$ simple   $\mathcal{G}$-measurable, so:
\begin{equation*}
	\mathbb{E}[V\cdot X|\mathcal{G}]=
	\sup_{n\geq 1}\mathbb{E}[V_n\cdot X|\mathcal{G}]=
	\sup_{n\geq 1}V_n\cdot \mathbb{E}[X|\mathcal{G}] =
	V\cdot \mathbb{E}[X|\mathcal{G}]
\end{equation*} 
If $V$ is real we can write $V=V^{+}-V^{-}$ and the thesis follows.
\end{proof}
\begin{teo}
If $X$ r.r.v. with $\mathbb{E}[|X|^2]<+\infty $, then:\\
$Z$ $\mathcal{G}$-measurable random variable with $\mathbb{E}[|Z|^2]<+\infty$\\
\begin{equation*}
	\mathbb{E}[|X-Z|^2]=\mathbb{E}[\ |X-\mathbb{E}[X|\mathcal{G}]|^2\ ]+\mathbb{E}[\ |\mathbb{E}[X|\mathcal{G}]-Z|^2\ ]
\end{equation*}
In particular:
\begin{equation*}
	\mathbb{E}[|X-Z|^2]=\min_{Z\in\mathcal{L}^2(\Omega,\mathcal{F},\mathbb{P})}\mathbb{E}[|X-\mathbb{E}[X|\mathcal{G}]|^2]
\end{equation*}
\end{teo}
\begin{proof}
	\begin{equation*}
		\begin{split}
			\mathbb{E}[|X-Z|^2]&=\mathbb{E}[\ [(X-\mathbb{E}[X|\mathcal{G}])+(\mathbb{E}[X|\mathcal{G}]-Z)]^2\ ]\\
			&=\mathbb{E}[\ |X-\mathbb{E}[X|\mathcal{G}]|^2\ ]+\mathbb{E}[\ |\mathbb{E}[X|\mathcal{G}]-Z|^2\ ]\\
			&\qquad+2\cdot\mathbb{E}[\ [(X-\mathbb{E}[X|\mathcal{G}])(\mathbb{E}[X|\mathcal{G}]-Z)]\ ]
		\end{split}		
	\end{equation*}
We only need to prove that the last term is zero.
By Projective property:
\begin{equation*}
\mathbb{E}[\ [(X-\mathbb{E}[X|\mathcal{G}])(\mathbb{E}[X|\mathcal{G}]-Z)]\ ]=
\mathbb{E}[\mathbb{E}[\ [(X-\mathbb{E}[X|\mathcal{G}])(\mathbb{E}[X|\mathcal{G}]-Z)]\ ]|\mathcal{G}]
\end{equation*}
$\mathbb{E}[X|\mathcal{G}]-Z$ is $\mathcal{G}$-measurable, so:
\begin{equation*}
	\mathbb{E}[\mathbb{E}[\ [(X-\mathbb{E}[X|\mathcal{G}])(\mathbb{E}[X|\mathcal{G}]-Z)]\ ]|\mathcal{G}]=
	\mathbb{E}[(\mathbb{E}[X|\mathcal{G}]-Z)\cdot\mathbb{E}[\ [(X-\mathbb{E}[X|\mathcal{G}])]\ ]|\mathcal{G}]
\end{equation*}
Finally:
\begin{equation*}
	\begin{split}
		\mathbb{E}[\ [(X-\mathbb{E}[X|\mathcal{G}])]\ |\mathcal{G}]&=\mathbb{E}[X|\mathcal{G}]-\mathbb{E}[\ [(\mathbb{E}[X|\mathcal{G}])]\ |\mathcal{G}]\\
		&=\mathbb{E}[X|\mathcal{G}]-\mathbb{E}[X|\mathcal{G}]=0
	\end{split}
\end{equation*}
\end{proof}
\end{enumerate}

\section{Martingales}
\subsection{Introduction}
$ (\Omega,\mathcal{F},\mathbb{P}) $ $T$ set of times either $\mathbb{N}$ or $[0,\infty)$.
\begin{defi}
	A \textbf{filtration} on the probability space $ (\Omega,\mathcal{F},\mathbb{P})$ is a sequence ${\mathcal{F}_{t}:t=0,1,2, \ldots}$ of sub-sigma algebras of $\mathcal{F}$, such that for all $t$, $\mathcal{F}_{t} \subseteq \mathcal{F}_{t+1}$.
\end{defi}
The filtration represent our knowledge at successive times. This increases with time (i.e. we don't forget things).
\begin{defi}
	$(M_t)_{t\in T}$ is a Martingale w.r.t. a filtration $(\mathcal{F}_t)_{t\in T}$ of sub-$\sigma$-algebras of $\mathcal{F}$ if:
	\begin{enumerate}
		\item \textbf{(adaptedness)} $M_t$ is $\mathcal{F}_t$-measurable $\forall t\in T$
		\item \textbf{(integrability)} $\mathbb{E}[|M_t|]<+\infty \qquad\forall t$
		\item \textbf{(martinagle property)} $\forall s<t \qquad \mathbb{E}[M_t|\mathcal{F}_s]=M_s$
	\end{enumerate}
\end{defi}
\begin{exi}
	$(X_n)_{n\geq0}$ symmetric RW on $\mathbb{Z}$ starting from $0$.\\
	$X_n=\sum_{k=1}^{n}Y_k \qquad Y_k$ iid with $\mathbb{P}\{Y_k=1\}=\frac{1}{2}=\mathbb{P}\{Y_k=-1\}$ \\
	$(X_n)_{n\geq0}$ is a martingale w.r.t. $\{\mathcal{F}_n\}_{n\geq0}$ with $\mathcal{F}_n=\sigma(X_1,...,X_n)$.
\end{exi}
\begin{proof}
	We check the definition.
	\begin{enumerate}
		\item $X_n$ is $\mathcal{F}_n$-measurable.
		\item $X_n$ is integrable $(|X_n|\leq n)$.
		\item If $m<n$,
		\begin{equation*}
			\mathbb{E}[X_n|\mathcal{F}_m]=\mathbb{E}\left[ \sum_{k=m+1}^{n}Y_k+\sum_{k=1}^{m}Y_k|\mathcal{F}_m \right]  
		\end{equation*}
		The first sum is independent of $\mathcal{F}_m$, the second one is $\mathcal{F}_n$-measurable, so:
		\begin{equation*}
			\mathbb{E}[X_n|\mathcal{F}_m]=\mathbb{E}\left[ \sum_{k=m+1}^{n}Y_k \right]  +\sum_{k=1}^{m}Y_k=\sum_{k=m+1}^{n}\mathbb{E}[Y_k]+X_m=X_m
		\end{equation*}
	Since $\mathbb{E}[Y_k]=0$.
	\end{enumerate}
\end{proof}
\begin{exi}[Another martingale]
	$M_n=X^2_n-n \qquad(M_n)_{n\geq 0}$
	\begin{enumerate}
		\item $M_n$ is $\mathcal{F}_n$-measurable.
		\item $|M_n|\leq|X^2_n-n|\leq n^2+n<+\infty\implies\mathbb{E}[|M_n|]<+\infty$.
		\item 
		\begin{equation*}
			\begin{split}
				\mathbb{E}[X_n|\mathcal{F}_m]&=\mathbb{E}[X^2_n-n|\mathcal{F}_m]\\
				&=\mathbb{E}[X^2_n|\mathcal{F}_m]-\mathbb{E}[n|\mathcal{F}_m]=\\
				&=\mathbb{E}\left[ \left( \sum_{k=1}^{m}Y_k+\sum_{k=m+1}^{n}Y_k \right)  ^2|\mathcal{F}_m \right]  -n=\\
				&=\mathbb{E}\left[ \left( \sum_{k=1}^{m}Y_k \right)  ^2|\mathcal{F}_m \right]  +2\cdot\mathbb{E}\left[ \left( \sum_{k=1}^{m}Y_k \right)  \cdot\left( \sum_{k=m+1}^{n}Y_k \right)  |\mathcal{F}_m \right]  \\
				& \qquad +\mathbb{E}\left[ \left( \sum_{k=m+1}^{n}Y_k \right)  ^2|\mathcal{F}_m \right]  -n=\\
				&=X^2_m+2\cdot X_m\cdot \mathbb{E}\left[ \sum_{k=m+1}^{n}Y_k|\mathcal{F}_m \right]  +\mathbb{E}\left[ \left( \sum_{k=m+1}^{n}Y_k \right)  ^2 \right]  -n=\\
				&=X^2_m+2\cdot X_m\cdot0+\sum_{k=m+1}^{n}\mathbb{E}[Y_k^2]-n=\\
				&=X^2_m+(n-m)-n=X^2_m-m=M_m
			\end{split}			
		\end{equation*}
	Since $\mathbb{E}[Y_k]=0$ and $\mathbb{E}[Y_k^2]=1$.
	\end{enumerate}
\end{exi}
\subsection{Stopping time}
$T$ stopping time of the filtration $(\mathcal{F}_t)_{t\in T}$ (i.e. $\{T\leq t\}\in\mathcal{F}_t \qquad\forall t\geq0$)\\
We call \textbf{stopped martingale at time} $T$: $M^{IT}=(M_{\min{(T,t)}})_{t\geq0}$.
\begin{teo}[Stopping Theorem]
	$(M_{\min{(T,t)}})_{t\geq0}$ is a martingale w.r.t. the same filtration $(\mathcal{F}_t)_{t\in T}$ and in particular:
	\begin{equation}
		\mathbb{E}[M_{\min{(T,t)}}]=\mathbb{E}[M_{0}]
	\end{equation}
\end{teo}
\begin{rem}
	A martingale $(M_t)_{t\in T}$ necesseraly has constant means by:
	\begin{equation*}
		\mathbb{E}[M_{T}]=\mathbb{E}[\mathbb{E}[M_{T}|\mathcal{F}_s]]=\mathbb{E}[M_{0}] \qquad \forall s<t
	\end{equation*} 
\end{rem}
\begin{exi}
	$(X_n)_{n\geq0}$ symmetric RW on $\mathbb{Z}$ starting from $0$ and $a,b\in\mathbb{N}$ or $a,b>0$.\\
	$X_n=\sum_{k=1}^{n}Y_k \qquad Y_k$ iid with $\mathbb{P}\{Y_k=1\}=\frac{1}{2}=\mathbb{P}\{Y_k=-1\}$ \\
	We want to compute the exit probability from $(-a,b)$ either from $-a$ or $b$ and the mean exit time $T=\min{(n\geq1|X_n=-a \text{ or } X_n=b)}$. \\
	Consider the martingales $(X_n)_{n\geq0}$ and $(X^2_n-n)_{n\geq0}$ and apply the stopping theorem: 
	\begin{equation*}
		\mathbb{E}[X^2_{T_{\Lambda}n}-T_{\Lambda}n]=\mathbb{E}[0]=0
	\end{equation*}
	\begin{equation}
		\mathbb{E}[X^2_{T_{\Lambda}n}]=\mathbb{E}[T_{\Lambda}n]
		\label{eq:applying-stopping-theorem-example}
	\end{equation}
Thus $ \mathbb{E}[T_{\Lambda}n]\leq\max{(a^2,b^2)} $ because $  |X^2_{T_{\Lambda}n}|\leq\max{(a^2,b^2)}$, thus we have: $\mathbb{E}[T]=\sup_{n\geq1}\mathbb{E}[T_{\Lambda}n]$\\
	Applying the stopping theorem to $(X_n)_{n\geq0}$:
	\begin{equation*}
		\mathbb{E}[X_{T_{\Lambda}n}]=\mathbb{E}[X_0]=0
	\end{equation*}
\begin{equation*}
	\mathbb{E}[X_{T_{\Lambda}n}]\leq\max{(a,b)}
\end{equation*}                                                 
Using dominated convergence:                                    
\begin{equation*}                                               
	\mathbb{E}[X_{T}]=\lim_{n\to\infty}\mathbb{E}[X_{T_{\Lambda}n}]=0
\end{equation*}
\begin{equation*}                                               
	0=-a\cdot\mathbb{P}\{X_T=-a\}+b\cdot\mathbb{P}\{X_T=b\}
\end{equation*}
Recalling that:
\begin{equation*}                                               
	\mathbb{P}\{X_T=-a\}+\mathbb{P}\{X_T=b\}=1
\end{equation*}
Solving the system:
\begin{equation*}                                               
	\mathbb{P}\{X_T=-a\}=\frac{b}{a+b} \qquad \mathbb{P}\{X_T=b\}=\frac{a}{a+b}
\end{equation*}
Then using monotone convergence on the RHS on \eqref{eq:applying-stopping-theorem-example} and dominated convergence on the LHS as $n\to\infty$ we obtain:
\begin{equation*}  
	\begin{split}
		\mathbb{E}[T]&=\mathbb{E}[X^2_T]=a^2\cdot\mathbb{P}\{X_T=-a\}+b^2\cdot\mathbb{P}\{X_T=b\}=\\&=\frac{a^2b}{a+b}+\frac{ab^2}{a+b}=ab
	\end{split}                                             
\end{equation*}
\end{exi}
\begin{rem}
	Note the resemblance of this result with the mean ending time starting from $i$ in the gambler's ruin problem:
	\begin{equation*}                                               
		\mathbb{E}_i[T]=i(N-i)
	\end{equation*}
\end{rem}
\begin{rem}[Warning]
	It is not always possible to take the limit $n\to\infty$ to find $ \mathbb{E}[M_{\min{(T,t)}}]=\mathbb{E}[M_{t}] $
\end{rem}
\begin{exi}
	$(X_n)_{n\geq0}$ symmetric RW on $\mathbb{Z}$ starting from $0$.\\
	$S=\min{\{n\geq1: X_n=1\}}$ \\
	Stopping theorem $\mathbb{E}[X_{\min{(S,n)}}]=0 $ but $\mathbb{E}[X_{S}]=1 $, therefore:
	\begin{equation*}
		\lim_{n\to\infty}\mathbb{E}[X_{\min{(S,n)}}]\neq\mathbb{E}[X_{S}]
	\end{equation*}
\end{exi}
\textbf{How to find martingales of a discrete time MC?}
\begin{teo}
	$(X_n)_{n\geq0}$ MC with values in a discrete set $I$\\
	$f:I\to\mathbb{R}\quad $ s.t. $ \mathbb{E}[|f(X_n)|]<+\infty$, then:
	\begin{equation}
		M_n=f(X_n)-\sum_{k=0}^{\textbf{n-1}}(Pf(X_k)-f(X_k))
	\end{equation}
$(M_n)_{n\geq0}$ is a martingale w.r.t. the natural filtration of $(X_n)_{n\geq0}$.
Where $(Pf)(j)=\sum_{j'\in I}p_{jj'}f(j')$.
\end{teo}
\begin{proof}
We check the definition:
	\begin{enumerate}
		\item $M_n$ is $\mathcal{F}_n$-measurable.
		\item $M_n$ is integrable.\\
		$\mathbb{E}[|f(X_n)|]<+\infty$ by assumption $\forall n$.\\
		$\mathbb{E}[|Pf(X_k)|]=\mathbb{E}[|\mathbb{E}[f(X_{k+1})|\mathcal{F}_k]|]<+\infty$
		The equality holds since:
		\begin{equation*}
			\mathbb{E}[f(X_{k+1})|\mathcal{F}_k]=Pf(X_k)
		\end{equation*}
	indeed for $m<n$:
	\begin{equation*}
		\mathbb{E}[f(X_{n})|\mathcal{F}_m]=P^{n-m}f(X_m)
	\end{equation*}
because:
		\begin{equation*}
			\begin{split}
				\mathbb{E}[f(X_{n})|\mathcal{F}_m]&=\mathbb{E}[\mathbb{E}[f(X_{n})|\mathcal{F}_{n-1}]|\mathcal{F}_m]\\&=\mathbb{E}[Pf(X_{n-1})|\mathcal{F}_m]\\&=\mathbb{E}[P^2f(X_{n-2})|\mathcal{F}_m]=...
			\end{split}		
	\end{equation*}
		\item For $n>m$:
		\begin{equation*}
			\begin{split}
			\mathbb{E}[M_{n}|\mathcal{F}_m]&=\mathbb{E}\left[ f(X_n)-\sum_{k=0}^{n-1}(Pf(X_k)-f(X_k))\bigg|\mathcal{F}_m \right]  \\
			&=\mathbb{E}\left[ f(X_n)-f(X_m)-\sum_{k=m}^{n-1}(Pf(X_k)-f(X_k))\bigg|\mathcal{F}_m \right]\\
			&\qquad +\underbrace{f(X_m)-\sum_{k=0}^{m-1}(Pf(X_k)-f(X_k))}_{M_m}\\
			&=(*)+M_m
			\end{split}		
		\end{equation*}
		The last term is $M_m$ so it suffices to show that the first term is zero. It is equal to:
		\begin{equation*}
			\begin{split}
				(*) &=\mathbb{E}\left[ \mathbb{E}\left[ f(X_n)-f(X_m)-\sum_{k=m}^{n-1}(Pf(X_k)-f(X_k))\bigg|\mathcal{F}_{n-1} \right]  \bigg|\mathcal{F}_m \right]  =\\
				&=\mathbb{E}\left[ \cancel{Pf(X_{n-1})}-f(X_m)-\cancel{Pf(X_{n-1})}+f(X_{n-1})-\sum_{k=m}^{n-2}(\ldots )\bigg|\mathcal{F}_m \right]  =\\
				&=\mathbb{E}\left[ f(X_{n-1})-f(X_m)-\sum_{k=m}^{n-2}(Pf(X_k)-f(X_k))\bigg|\mathcal{F}_m \right]  =\\
				&=\text{iterating...}=\mathbb{E}[f(X_m)-f(X_m)]=0
			\end{split}		
		\end{equation*}
	\end{enumerate}
\end{proof}
\begin{exi}
	$(X_n)_{n\geq0}$ symmetric RW on $\mathbb{Z}$ starting from $0$.\\
	$f(x)=x^2$.\\
	$\left(M_n=f(X_n)-\sum_{k=1}^{n-1}\left(Pf(X_k)-f(X_k)\right)\right)$
	\begin{equation*}
		Pf(i)=p_{i,i+1}f(i+1)+p_{i,i-1}f(i-1)=\frac{1}{2}(i+1)^2+\frac{1}{2}(i-1)^2=i^2+1
	\end{equation*}
\begin{equation*}
	Pf(i)-f(i)=1
\end{equation*}
\begin{equation*}
	\sum_{k=0}^{n-1}Pf(i)-f(i)=n \implies M_n=X_n^2-n
\end{equation*}
\end{exi}
\begin{rem}
	If you add a constant to a martingale it remains a martingale.
\end{rem}
\end{document}